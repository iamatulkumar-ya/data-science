{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d9f97b",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Post initial text preprocessing, we need to transform the text into a meaningful vector of numbers such that a model can perform an operation on the same. There are several techniques to achieve these. Few popular ones are:\n",
    "\n",
    "1. Bag of Words\n",
    "2. TF- IDF (Term Frequencey - Inverse Document Frequency)\n",
    "3. N-Grams Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ba538",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "term frequence - inverse document frequency.\n",
    "\n",
    "tf: term frequency: how many times a word appears into the single document\n",
    "\n",
    "tf: count of word appreance / len(single document words)\n",
    "\n",
    "\n",
    "idf: inverse document frequency; in how many documents a word appears \n",
    "\n",
    "idf: log(total documents / total appreance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1e3bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8f4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = ['Be yourself; everyone else is already taken.',\n",
    " 'The biggest adventure you can take is to live the life of your dreams.',\n",
    " 'The only thing we have to fear is fear itself.',\n",
    " 'Some people want it to happen, some wish it would happen, others make it happen.',\n",
    " 'You’ve got to be in it to win it.',\n",
    " 'It does not matter how slowly you go, as long as you do not stop.',\n",
    " 'Find out who you are and do it on purpose.',\n",
    " 'For me, becoming isn’t about arriving somewhere or achieving a certain aim. I see it instead as forward motion, a means of evolving, a way to reach continuously toward a better self. The journey doesn’t end.',\n",
    " 'Confident people have a way of carrying themselves that makes others attracted to them.',\n",
    " 'If you can do what you do best and be happy, you are further along in life than most people.',\n",
    " 'You can be everything. You can be the infinite amount of things that people are.',\n",
    " 'Always go with your passions. Never ask yourself if it’s realistic or not.',\n",
    " 'When you change your thoughts, remember to also change your world.',\n",
    " 'The more you know who you are, and what you want, the less you let things upset you.',\n",
    " 'By being yourself, you put something wonderful in the world that was not there before.',\n",
    " 'Do one thing every day that scares you.',\n",
    " 'It is never too late to be what you might have been.',\n",
    " 'Find out who you are and be that person. That’s what your soul was put on this earth to be. Find the truth, live that truth, and everything else will come.',\n",
    " 'When we are no longer able to change a situation, we are challenged to change ourselves.',\n",
    " 'If you cannot do great things, do small things in a great way.',\n",
    " 'Always do your best. What you plant now, you will harvest later.',\n",
    " 'Get busy living or get busy dying.',\n",
    " 'In three words I can sum up everything I’ve learned about life: It goes on.',\n",
    " 'You can’t help what you feel, but you can help how you behave.',\n",
    " 'No need to hurry. No need to sparkle. No need to be anybody but oneself.',\n",
    " 'Promise me you’ll always remember: You’re braver than you believe, and stronger than you seem, and smarter than you think.',\n",
    " 'Failure is a great teacher and, if you are open to it, every mistake has a lesson to offer.',\n",
    " 'If you don’t like the road you’re walking, start paving another one.',\n",
    " 'Don’t let yesterday take up too much of today.',\n",
    " 'Keep smiling, because life is a beautiful thing and there’s so much to smile about.',\n",
    " 'Be persistent and never give up hope.',\n",
    " 'When we strive to become better than we are, everything around us becomes better too.',\n",
    " 'Believe and act as if it were impossible to fail.',\n",
    " 'There are so many great things in life; why dwell on negativity?',\n",
    " 'Happiness often sneaks in through a door you didn’t know you left open.',\n",
    " 'Always remember that you are absolutely unique. Just like everyone else.',\n",
    " 'Keep your face towards the sunshine and shadows will fall behind you.',\n",
    " 'A problem is a chance for you to do your best.',\n",
    " 'You don’t always need a plan. Sometimes you just need to breathe, trust, let go and see what happens.',\n",
    " 'Nothing is impossible. The word itself says ‘I’m possible!’',\n",
    " 'Life does not have to be perfect to be wonderful.',\n",
    " 'It is during our darkest moments that we must focus to see the light.',\n",
    " 'The best way out is through.',\n",
    " 'Don’t be afraid to give up the good to go for the great.',\n",
    " 'Whether you think you can or you can’t, you’re right.',\n",
    " 'Don’t take yourself too seriously. Know when to laugh at yourself, and find a way to laugh at obstacles that inevitably present themselves.',\n",
    " 'Love the life you live. Live the life you love.',\n",
    " 'Keep your face towards the sunshine and shadows will fall behind you.',\n",
    " 'The only person you are destined to become is the person you decide to be.',\n",
    " 'I’m not going to continue knocking that old door that doesn’t open for me. I’m going to create my own door and walk through that.',\n",
    " 'If you change the way you look at things, the things you look at change.',\n",
    " 'I believe that if you’ll just stand up and go, life will open up for you. Something just motivates you to keep moving.',\n",
    " 'Once you face your fear, nothing is ever as hard as you think.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85bb181",
   "metadata": {},
   "source": [
    "let's cleanup the text little bit, ideally we must perform complete preprocessing for better results. However for learning we can just do a bit of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8df5cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Be yourself everyone else is already taken',\n",
       " 'The biggest adventure you can take is to live the life of your dreams',\n",
       " 'The only thing we have to fear is fear itself',\n",
       " 'Some people want it to happen some wish it would happen others make it happen',\n",
       " 'You’ve got to be in it to win it']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedQuotes = []\n",
    "for q in quotes:\n",
    "    for p in string.punctuation:\n",
    "        q = q.replace(p, \"\")\n",
    "\n",
    "    cleanedQuotes.append(q)\n",
    "\n",
    "cleanedQuotes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8c571",
   "metadata": {},
   "source": [
    "let's compute some metadata that will help us with tf-idf computaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7e7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# unique words count\n",
    "uniqueWords = list(set(\" \".join(cleanedQuotes).split()))\n",
    "print(len(uniqueWords))\n",
    "\n",
    "totalDocumentCount = len(cleanedQuotes)\n",
    "print(totalDocumentCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ab19a",
   "metadata": {},
   "source": [
    "let's calculate tf-idf and form the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21504ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSpaces = []\n",
    "\n",
    "\n",
    "for quote in cleanedQuotes:\n",
    "    crr_quote_list = quote.split()\n",
    "    vectorSpace = []\n",
    "    for uw in uniqueWords: \n",
    "        if uw in crr_quote_list:\n",
    "            # tf - term frequency: how many times a word came into the single document\n",
    "            tf = quote.count(uw) / len(crr_quote_list)\n",
    "\n",
    "            # idf - inverse document frequency; in how many documents a word appears \n",
    "            wordAppearedCount = len([q for q in cleanedQuotes if q.count(uw) > 0])\n",
    "            idf = math.log10(totalDocumentCount/wordAppearedCount)\n",
    "\n",
    "            vectorSpace.append(tf*idf)\n",
    "        else:\n",
    "            vectorSpace.append(0)\n",
    "    vectorSpaces.append({quote:vectorSpace})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfa0fd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Be yourself everyone else is already taken': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.16031655403897524,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.17816494498301808,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.24632512422868413,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.24632512422868413,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.2033208391338297,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.06700048064249757,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.17816494498301808,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]},\n",
       " {'The biggest adventure you can take is to live the life of your dreams': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.12316256211434207,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.062798416399038,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.062798416399038,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.07323613323319787,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.12316256211434207,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.01979413130418356,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.05865613447206039,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.030231848138343415,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.04607818739665459,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.08908247249150904,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.062798416399038,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.03327345368093091,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.12316256211434207,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.033500240321248785,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look into some data\n",
    "vectorSpaces[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847a732",
   "metadata": {},
   "source": [
    "let's define the definiion to compute cosine similarities between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db262381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two NumPy arrays (vectors).\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    magnitude_a = np.linalg.norm(vector_a)\n",
    "    magnitude_b = np.linalg.norm(vector_b)\n",
    "\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0  # Handle cases where one or both vectors are zero vectors\n",
    "\n",
    "    cosine_similarity = dot_product / (magnitude_a * magnitude_b)\n",
    "    return cosine_similarity\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ef9ed",
   "metadata": {},
   "source": [
    "Now, let's give some partial quote and see what is the actual code which matches with the vector cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6611079",
   "metadata": {},
   "outputs": [],
   "source": [
    "partialQuote = \"Always remember\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc831d14",
   "metadata": {},
   "source": [
    "to get the similar quote, we have to first compute the vector space for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcedc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6235773074405633, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6235773074405633, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crr_quote_list = partialQuote.split()\n",
    "vectorSpace = []\n",
    "for uw in uniqueWords: \n",
    "    if uw in crr_quote_list:\n",
    "    \n",
    "        # tf - term frequency: how many times a word came into the single document\n",
    "        tf = partialQuote.count(uw) / len(crr_quote_list)\n",
    "\n",
    "        # idf - inverse document frequency; in how many documents a word appears \n",
    "        wordAppearedCount = len([q for q in cleanedQuotes if q.count(uw) > 0])\n",
    "        idf = math.log10(totalDocumentCount/wordAppearedCount)\n",
    "\n",
    "        vectorSpace.append(tf*idf)\n",
    "\n",
    "    else:\n",
    "        vectorSpace.append(0)\n",
    "\n",
    "queryVector = vectorSpace\n",
    "print(queryVector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803f668",
   "metadata": {},
   "source": [
    "now, let's compute cosine similarity with new vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "800937da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimilarities = []\n",
    "\n",
    "for vs in vectorSpaces: \n",
    "    cosineSimilarities.append((list(vs.keys())[0], calculate_cosine_similarity(queryVector, list(vs.values())[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4041342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Be yourself everyone else is already taken', np.float64(0.0)),\n",
       " ('The biggest adventure you can take is to live the life of your dreams',\n",
       "  np.float64(0.0))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarities[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8467357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec047a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "heapq.heapify(cosineSimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7fabb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Always remember that you are absolutely unique Just like everyone else',\n",
       "  np.float64(0.40860901827254376)),\n",
       " ('Always do your best What you plant now you will harvest later',\n",
       "  np.float64(0.20639354586651698)),\n",
       " ('When you change your thoughts remember to also change your world',\n",
       "  np.float64(0.19953133095807818))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting 3 quotes that are matching with the partial quote\n",
    "matchedQuotes = heapq.nlargest(3, cosineSimilarities, key=lambda x: x[1] )\n",
    "matchedQuotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e30299ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For given partial quote - 'Always remember', below quote is matching with cosine similaity as 0.40860901827254376 \n",
      " 'Always remember that you are absolutely unique Just like everyone else'\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "\n",
    "print(f\"For given partial quote - '{partialQuote}', below quote is matching with cosine similaity as {matchedQuotes[0][1]} \\n '{matchedQuotes[0][0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dab54e",
   "metadata": {},
   "source": [
    "like wise, we can build, searching or text suggestion etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
