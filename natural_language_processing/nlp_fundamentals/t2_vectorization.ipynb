{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d9f97b",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Post initial text preprocessing, we need to transform the text into a meaningful vector of numbers such that a model can perform an operation on the same. There are several techniques to achieve these. Few popular ones are:\n",
    "\n",
    "1. Bag of Words\n",
    "2. TF- IDF (Term Frequencey - Inverse Document Frequency)\n",
    "3. N-Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1e3bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8080208",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "a method for representing text data by counting the occurrences of words within a document, disregarding grammar and word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8f4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A kind slave ran away from his cruel master and hid in the forest. There, he saw a lion roaring in pain because a big thorn was stuck in its paw. Even though he was scared, the slave helped the lion by pulling the thorn out. The lion went back into the woods, free and happy. Later, the slave was caught and sent to be punished by being thrown into a lion’s den. But the lion didn’t harm him—it was the same lion he had helped! Moral of the story: A good deed is never forgotten. Be kind, and kindness will come back to you.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85bb181",
   "metadata": {},
   "source": [
    "let's cleanup the text little bit, ideally we must perform complete preprocessing for better results. However for learning we can just do a bit of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8df5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\".\", \"SENTBREAKER\")\n",
    "for p in string.punctuation:\n",
    "    text = text.replace(p, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7e7b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A kind slave ran away from his cruel master and hid in the forestSENTBREAKER There he saw a lion roaring in pain because a big thorn was stuck in its pawSENTBREAKER Even though he was scared the slave helped the lion by pulling the thorn outSENTBREAKER The lion went back into the woods free and happySENTBREAKER Later the slave was caught and sent to be punished by being thrown into a lion’s denSENTBREAKER But the lion didn’t harm him—it was the same lion he had helped Moral of the story A good deed is never forgottenSENTBREAKER Be kind and kindness will come back to youSENTBREAKER'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look into the text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c5df2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A kind slave ran away from his cruel master and hid in the forest',\n",
       " 'There he saw a lion roaring in pain because a big thorn was stuck in its paw',\n",
       " 'Even though he was scared the slave helped the lion by pulling the thorn out',\n",
       " 'The lion went back into the woods free and happy',\n",
       " 'Later the slave was caught and sent to be punished by being thrown into a lion’s den',\n",
       " 'But the lion didn’t harm him—it was the same lion he had helped Moral of the story A good deed is never forgotten',\n",
       " 'Be kind and kindness will come back to you']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a sentences to understand the concept. If we take word as token then we can not derive anything from that.\n",
    "textSentenceToken = [item.strip() for item in text.split(\"SENTBREAKER\") if item]\n",
    "textSentenceToken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5eb36",
   "metadata": {},
   "source": [
    "let's take the unique words within dataset, to determine the vector for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60b7d5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords = list(set(text.split()))\n",
    "len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b81468d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Later',\n",
       " 'come',\n",
       " 'to',\n",
       " 'Even',\n",
       " 'slave',\n",
       " 'hid',\n",
       " 'never',\n",
       " 'was',\n",
       " 'will',\n",
       " 'kindness']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look few uniquewords\n",
    "uniqueWords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededfa38",
   "metadata": {},
   "source": [
    "let's compute the vector by computing the occurrance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e006aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSpaces = []\n",
    "# looping through each sentence\n",
    "for sentence in textSentenceToken:\n",
    "    vectorSpace = {}\n",
    "    sentenceWordList = sentence.split()\n",
    "    for uw in uniqueWords:\n",
    "        if uw in sentenceWordList:\n",
    "            if uw not in vectorSpace:\n",
    "                vectorSpace[uw] = 1\n",
    "            else:\n",
    "                vectorSpace[uw] += 1\n",
    "        else:\n",
    "            vectorSpace[uw] = 0\n",
    "\n",
    "    vectorSpaces.append(list(vectorSpace.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "561d0c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look into the vectors\n",
    "vectorSpaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04250b",
   "metadata": {},
   "source": [
    "let's visualize with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "575b7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=vectorSpaces, columns=uniqueWords,  index=textSentenceToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "991e90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(os.getcwd(), \"datafiles\", \"bag_of_wrods_result.csv\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9708062",
   "metadata": {},
   "source": [
    "Drawback: Vector size grows with the text. High Dimensonality\n",
    "\n",
    "It does not put the weight on the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
