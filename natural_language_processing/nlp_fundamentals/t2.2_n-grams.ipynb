{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d9f97b",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Post initial text preprocessing, we need to transform the text into a meaningful vector of numbers such that a model can perform an operation on the same. There are several techniques to achieve these. Few popular ones are:\n",
    "\n",
    "1. Bag of Words\n",
    "2. TF- IDF (Term Frequencey - Inverse Document Frequency)\n",
    "3. N-Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1e3bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619c499",
   "metadata": {},
   "source": [
    "#### N-Grams\n",
    "N-grams are contiguous sequences of n item (words or characters) from a given text. There are multiple types of grams that we can implement such as:\n",
    "1. Unigram (1-gram): individual words\n",
    "2. Bigram (2-gram) : pairs of consecutive words\n",
    "3. Trigram (3-gram) : triples of consecutive words and so on..\n",
    "\n",
    "N-grams helps in text classification, machine learning and transaltion task etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e500f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take the dataset\n",
    "\n",
    "quotes = ['Be yourself; everyone else is already taken.',\n",
    " 'The biggest adventure you can take is to live the life of your dreams.',\n",
    " 'The only thing we have to fear is fear itself.',\n",
    " 'Some people want it to happen, some wish it would happen, others make it happen.',\n",
    " 'You’ve got to be in it to win it.',\n",
    " 'It does not matter how slowly you go, as long as you do not stop.',\n",
    " 'Find out who you are and do it on purpose.',\n",
    " 'For me, becoming isn’t about arriving somewhere or achieving a certain aim. I see it instead as forward motion, a means of evolving, a way to reach continuously toward a better self. The journey doesn’t end.',\n",
    " 'Confident people have a way of carrying themselves that makes others attracted to them.',\n",
    " 'If you can do what you do best and be happy, you are further along in life than most people.',\n",
    " 'You can be everything. You can be the infinite amount of things that people are.',\n",
    " 'Always go with your passions. Never ask yourself if it’s realistic or not.',\n",
    " 'When you change your thoughts, remember to also change your world.',\n",
    " 'The more you know who you are, and what you want, the less you let things upset you.',\n",
    " 'By being yourself, you put something wonderful in the world that was not there before.',\n",
    " 'Do one thing every day that scares you.',\n",
    " 'It is never too late to be what you might have been.',\n",
    " 'Find out who you are and be that person. That’s what your soul was put on this earth to be. Find the truth, live that truth, and everything else will come.',\n",
    " 'When we are no longer able to change a situation, we are challenged to change ourselves.',\n",
    " 'If you cannot do great things, do small things in a great way.',\n",
    " 'Always do your best. What you plant now, you will harvest later.',\n",
    " 'Get busy living or get busy dying.',\n",
    " 'In three words I can sum up everything I’ve learned about life: It goes on.',\n",
    " 'You can’t help what you feel, but you can help how you behave.',\n",
    " 'No need to hurry. No need to sparkle. No need to be anybody but oneself.',\n",
    " 'Promise me you’ll always remember: You’re braver than you believe, and stronger than you seem, and smarter than you think.',\n",
    " 'Failure is a great teacher and, if you are open to it, every mistake has a lesson to offer.',\n",
    " 'If you don’t like the road you’re walking, start paving another one.',\n",
    " 'Don’t let yesterday take up too much of today.',\n",
    " 'Keep smiling, because life is a beautiful thing and there’s so much to smile about.',\n",
    " 'Be persistent and never give up hope.',\n",
    " 'When we strive to become better than we are, everything around us becomes better too.',\n",
    " 'Believe and act as if it were impossible to fail.',\n",
    " 'There are so many great things in life; why dwell on negativity?',\n",
    " 'Happiness often sneaks in through a door you didn’t know you left open.',\n",
    " 'Always remember that you are absolutely unique. Just like everyone else.',\n",
    " 'Keep your face towards the sunshine and shadows will fall behind you.',\n",
    " 'A problem is a chance for you to do your best.',\n",
    " 'You don’t always need a plan. Sometimes you just need to breathe, trust, let go and see what happens.',\n",
    " 'Nothing is impossible. The word itself says ‘I’m possible!’',\n",
    " 'Life does not have to be perfect to be wonderful.',\n",
    " 'It is during our darkest moments that we must focus to see the light.',\n",
    " 'The best way out is through.',\n",
    " 'Don’t be afraid to give up the good to go for the great.',\n",
    " 'Whether you think you can or you can’t, you’re right.',\n",
    " 'Don’t take yourself too seriously. Know when to laugh at yourself, and find a way to laugh at obstacles that inevitably present themselves.',\n",
    " 'Love the life you live. Live the life you love.',\n",
    " 'Keep your face towards the sunshine and shadows will fall behind you.',\n",
    " 'The only person you are destined to become is the person you decide to be.',\n",
    " 'I’m not going to continue knocking that old door that doesn’t open for me. I’m going to create my own door and walk through that.',\n",
    " 'If you change the way you look at things, the things you look at change.',\n",
    " 'I believe that if you’ll just stand up and go, life will open up for you. Something just motivates you to keep moving.',\n",
    " 'Once you face your fear, nothing is ever as hard as you think.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5be6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Optimus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36bad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Optimus/nltk_data'\n    - 'd:\\\\ProjectDesk\\\\gh-iamatulkumar-ya\\\\data-science\\\\.venv\\\\nltk_data'\n    - 'd:\\\\ProjectDesk\\\\gh-iamatulkumar-ya\\\\data-science\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\ProjectDesk\\\\gh-iamatulkumar-ya\\\\data-science\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Optimus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Download necessary NLTK resources (if not already downloaded)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# nltk.download('punkt')\u001b[39;00m\n\u001b[32m      8\u001b[39m text_data = \u001b[33m\"\u001b[39m\u001b[33mthis is a very good book to study\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m words = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Generate bigrams\u001b[39;00m\n\u001b[32m     12\u001b[39m bigrams_nltk = \u001b[38;5;28mlist\u001b[39m(ngrams(words, \u001b[32m2\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProjectDesk\\gh-iamatulkumar-ya\\data-science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProjectDesk\\gh-iamatulkumar-ya\\data-science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProjectDesk\\gh-iamatulkumar-ya\\data-science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProjectDesk\\gh-iamatulkumar-ya\\data-science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProjectDesk\\gh-iamatulkumar-ya\\data-science\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProjectDesk\\gh-iamatulkumar-ya\\data-science\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Optimus/nltk_data'\n    - 'd:\\\\ProjectDesk\\\\gh-iamatulkumar-ya\\\\data-science\\\\.venv\\\\nltk_data'\n    - 'd:\\\\ProjectDesk\\\\gh-iamatulkumar-ya\\\\data-science\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\ProjectDesk\\\\gh-iamatulkumar-ya\\\\data-science\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Optimus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "text_data = \"this is a very good book to study\"\n",
    "words = nltk.word_tokenize(text_data)  # Tokenize the text\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams_nltk = list(ngrams(words, 2))\n",
    "print(f\"NLTK Bigrams: {bigrams_nltk}\")\n",
    "\n",
    "# Count frequency of bigrams\n",
    "bigram_freq = Counter(bigrams_nltk)\n",
    "print(f\"Bigram Frequencies: {bigram_freq.most_common(5)}\") # Top 5 most common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85d8da",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0c927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
